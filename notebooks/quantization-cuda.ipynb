{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b0b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/music/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c79050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docstring</th>\n",
       "      <th>code</th>\n",
       "      <th>function</th>\n",
       "      <th>file_path</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fetches information on multiple orders made by...</td>\n",
       "      <td>def fetch_orders(self, symbol: Str = None, sin...</td>\n",
       "      <td>oceanex.fetch_orders</td>\n",
       "      <td>python/ccxt/oceanex.py</td>\n",
       "      <td>oceanex.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test if the accelerator is set to `tpu` when d...</td>\n",
       "      <td>def test_accelerator_set_when_using_tpu(device...</td>\n",
       "      <td>test_accelerator_set_when_using_tpu</td>\n",
       "      <td>tests/tests_pytorch/models/test_tpu.py</td>\n",
       "      <td>test_tpu.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Initialize data storage.</td>\n",
       "      <td>def __init__(self, hass: HomeAssistant, legacy...</td>\n",
       "      <td>StoredData.__init__</td>\n",
       "      <td>homeassistant/components/feedreader/__init__.py</td>\n",
       "      <td>__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fetches the history of funding rates\\n        ...</td>\n",
       "      <td>def fetch_funding_rate_history(self, symbol=No...</td>\n",
       "      <td>bitmex.fetch_funding_rate_history</td>\n",
       "      <td>python/ccxt/bitmex.py</td>\n",
       "      <td>bitmex.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see https://www.bitmex.com/api/explorer/#not /...</td>\n",
       "      <td>def fetch_my_trades(self, symbol: Optional[str...</td>\n",
       "      <td>bitmex.fetch_my_trades</td>\n",
       "      <td>python/ccxt/bitmex.py</td>\n",
       "      <td>bitmex.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>Inspect any Python object.\\n\\n    * inspect(&lt;O...</td>\n",
       "      <td>def inspect(\\n    obj: Any,\\n    *,\\n    conso...</td>\n",
       "      <td>inspect</td>\n",
       "      <td>rich/__init__.py</td>\n",
       "      <td>__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>Pretty prints JSON. Output will be valid JSON....</td>\n",
       "      <td>def print_json(\\n    json: Optional[str] = Non...</td>\n",
       "      <td>print_json</td>\n",
       "      <td>rich/__init__.py</td>\n",
       "      <td>__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>Return true if safety issue detected.</td>\n",
       "      <td>def is_on(self):\\n        \\n        parent_is_...</td>\n",
       "      <td>HomematicipSecuritySensorGroup.is_on</td>\n",
       "      <td>homeassistant/components/homematicip_cloud/bin...</td>\n",
       "      <td>binary_sensor.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>validates an input instance before a convex-hu...</td>\n",
       "      <td>def _validate_input(points):\\n    \\n    \\n    ...</td>\n",
       "      <td>_validate_input</td>\n",
       "      <td>divide_and_conquer/convex_hull.py</td>\n",
       "      <td>convex_hull.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>Test initialize gRPCProxyRequest with healthz ...</td>\n",
       "      <td>def test_calling_healthz_method(self):\\n      ...</td>\n",
       "      <td>TestgRPCProxyRequest.test_calling_healthz_method</td>\n",
       "      <td>python/ray/serve/tests/unit/test_proxy_request...</td>\n",
       "      <td>test_proxy_request_response.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              docstring  \\\n",
       "0     fetches information on multiple orders made by...   \n",
       "1     Test if the accelerator is set to `tpu` when d...   \n",
       "2                              Initialize data storage.   \n",
       "3     Fetches the history of funding rates\\n        ...   \n",
       "4     see https://www.bitmex.com/api/explorer/#not /...   \n",
       "...                                                 ...   \n",
       "2295  Inspect any Python object.\\n\\n    * inspect(<O...   \n",
       "2296  Pretty prints JSON. Output will be valid JSON....   \n",
       "2297              Return true if safety issue detected.   \n",
       "2298  validates an input instance before a convex-hu...   \n",
       "2299  Test initialize gRPCProxyRequest with healthz ...   \n",
       "\n",
       "                                                   code  \\\n",
       "0     def fetch_orders(self, symbol: Str = None, sin...   \n",
       "1     def test_accelerator_set_when_using_tpu(device...   \n",
       "2     def __init__(self, hass: HomeAssistant, legacy...   \n",
       "3     def fetch_funding_rate_history(self, symbol=No...   \n",
       "4     def fetch_my_trades(self, symbol: Optional[str...   \n",
       "...                                                 ...   \n",
       "2295  def inspect(\\n    obj: Any,\\n    *,\\n    conso...   \n",
       "2296  def print_json(\\n    json: Optional[str] = Non...   \n",
       "2297  def is_on(self):\\n        \\n        parent_is_...   \n",
       "2298  def _validate_input(points):\\n    \\n    \\n    ...   \n",
       "2299  def test_calling_healthz_method(self):\\n      ...   \n",
       "\n",
       "                                              function  \\\n",
       "0                                 oceanex.fetch_orders   \n",
       "1                  test_accelerator_set_when_using_tpu   \n",
       "2                                  StoredData.__init__   \n",
       "3                    bitmex.fetch_funding_rate_history   \n",
       "4                               bitmex.fetch_my_trades   \n",
       "...                                                ...   \n",
       "2295                                           inspect   \n",
       "2296                                        print_json   \n",
       "2297              HomematicipSecuritySensorGroup.is_on   \n",
       "2298                                   _validate_input   \n",
       "2299  TestgRPCProxyRequest.test_calling_healthz_method   \n",
       "\n",
       "                                              file_path  \\\n",
       "0                                python/ccxt/oceanex.py   \n",
       "1                tests/tests_pytorch/models/test_tpu.py   \n",
       "2       homeassistant/components/feedreader/__init__.py   \n",
       "3                                 python/ccxt/bitmex.py   \n",
       "4                                 python/ccxt/bitmex.py   \n",
       "...                                                 ...   \n",
       "2295                                   rich/__init__.py   \n",
       "2296                                   rich/__init__.py   \n",
       "2297  homeassistant/components/homematicip_cloud/bin...   \n",
       "2298                  divide_and_conquer/convex_hull.py   \n",
       "2299  python/ray/serve/tests/unit/test_proxy_request...   \n",
       "\n",
       "                            filename  \n",
       "0                         oceanex.py  \n",
       "1                        test_tpu.py  \n",
       "2                        __init__.py  \n",
       "3                          bitmex.py  \n",
       "4                          bitmex.py  \n",
       "...                              ...  \n",
       "2295                     __init__.py  \n",
       "2296                     __init__.py  \n",
       "2297                binary_sensor.py  \n",
       "2298                  convex_hull.py  \n",
       "2299  test_proxy_request_response.py  \n",
       "\n",
       "[2300 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = {\n",
    "    'test': '../data/raw/test.jsonl',\n",
    "    'train': '../data/raw/train.jsonl'\n",
    "}\n",
    "\n",
    "def readDataset(file):\n",
    "    test_set, train_set = [], []\n",
    "    # Open and parse the JSONL file\n",
    "    with open(file, 'r') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "\n",
    "    for data_point in dataset:\n",
    "        test_set.append({\n",
    "            'docstring': data_point['version_data'][-1]['docstring'],\n",
    "            'code': data_point['version_data'][-1]['code'],\n",
    "            'function': data_point['function'],\n",
    "            'file_path': data_point['file_path'],\n",
    "            'filename': data_point['filename'],\n",
    "        })\n",
    "    return pd.DataFrame(test_set)\n",
    "\n",
    "test_df, train_df = pd.DataFrame(), pd.DataFrame()\n",
    "for name, path in files.items():\n",
    "    if name == 'test':\n",
    "        test_df = readDataset(path)\n",
    "    elif name == 'train':\n",
    "        train_df = readDataset(path)\n",
    "\n",
    "test_df\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd51961d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    211\u001b[39m dataset_path = \u001b[33m\"\u001b[39m\u001b[33mpath/to/your/code_comment_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# You can use a larger model with these memory savings\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# \"Salesforce/codet5p-770m\" or even \"Salesforce/codet5p-2b\" \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m model, tokenizer = \u001b[43mfinetune_code_comment_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSalesforce/codet5p-770m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./code-comment-lora-model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    219\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Load and use the model for inference\u001b[39;00m\n\u001b[32m    222\u001b[39m load_and_use_model(\u001b[33m\"\u001b[39m\u001b[33m./code-comment-lora-model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mfinetune_code_comment_model\u001b[39m\u001b[34m(dataset_path, model_name, output_dir)\u001b[39m\n\u001b[32m     56\u001b[39m quantization_config = BitsAndBytesConfig(\n\u001b[32m     57\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     58\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m     59\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     60\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m )\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Load model with quantization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m model = \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Prepare model for k-bit training\u001b[39;00m\n\u001b[32m     71\u001b[39m model = prepare_model_for_kbit_training(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/music/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    568\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/music/lib/python3.12/site-packages/transformers/modeling_utils.py:262\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    264\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/music/lib/python3.12/site-packages/transformers/modeling_utils.py:3698\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3695\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3705\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   3706\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/music/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:75\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     72\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     )\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# 1. Load and prepare your dataset\n",
    "def load_dataset(file_path):\n",
    "    test_file = '../data/raw/test.jsonl'\n",
    "    train_file = '../data/raw/train.jsonl'\n",
    "\n",
    "    test_df = pd.read_json(test_file, lines=True)\n",
    "    train_df = pd.read_json(train_file, lines=True)    \n",
    "\n",
    "    # # Split into train/validation/test\n",
    "    # train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    # train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Convert to Hugging Face datasets\n",
    "    # train_dataset = Dataset.from_pandas(train_df)\n",
    "    # val_dataset = Dataset.from_pandas(val_df)\n",
    "    # test_dataset = Dataset.from_pandas(test_df)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# 2. Tokenization functions\n",
    "def preprocess_function(examples, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    inputs = examples['diff_code']\n",
    "    targets = examples['diff_docstring']\n",
    "    \n",
    "    # Add a prefix to help the model understand the task\n",
    "    model_inputs = tokenizer(\n",
    "        [\"Generate a comment for this code: \" + code for code in inputs],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 3. Main fine-tuning function with LoRA and quantization\n",
    "def finetune_code_comment_model(dataset_path, model_name=\"Salesforce/codet5p-220m\", output_dir=\"./code-comment-model\"):\n",
    "    # Load datasets\n",
    "    train_dataset, val_dataset, test_dataset = load_dataset(dataset_path)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Configure quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,  # rank\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        # Target modules depend on the model architecture\n",
    "        # For CodeT5, we typically want to adapt attention layers\n",
    "        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi\", \"wo\"],\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Preprocess datasets\n",
    "    tokenized_train = train_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer),\n",
    "        batched=True\n",
    "    )\n",
    "    tokenized_val = val_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer),\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        pad_to_multiple_of=8\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",  # Match evaluation_strategy\n",
    "        learning_rate=1e-4,  # Slightly higher learning rate for LoRA\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,  # Further reduce memory requirements\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=3,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,  # Mixed precision training\n",
    "        report_to=\"tensorboard\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the adapter model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating fine-tuned model...\")\n",
    "    test_results = evaluate_model(model, tokenizer, test_dataset)\n",
    "    print(f\"Test Results: {test_results}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 4. Evaluation function\n",
    "def evaluate_model(model, tokenizer, test_dataset):\n",
    "    # A simple evaluation function\n",
    "    def generate_comment(code):\n",
    "        inputs = tokenizer(\"Generate a comment for this code: \" + code, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Generate comments for a few examples\n",
    "    examples = test_dataset.select(range(min(5, len(test_dataset))))\n",
    "    for i, example in enumerate(examples):\n",
    "        code = example['diff_code']\n",
    "        expected = example['diff_docstring']\n",
    "        generated = generate_comment(code)\n",
    "        \n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Code: {code[:100]}...\")\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Here you would implement proper evaluation metrics\n",
    "    return {\"status\": \"Evaluation complete\"}\n",
    "\n",
    "# 5. Inference with the trained model\n",
    "def load_and_use_model(model_path):\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # For inference, we can load in 8-bit to save even more memory\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    # Example usage\n",
    "    code_snippet = \"\"\"\n",
    "    def calculate_average(numbers):\n",
    "        total = sum(numbers)\n",
    "        return total / len(numbers)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate comment\n",
    "    inputs = tokenizer(\"Generate a comment for this code: \" + code_snippet, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=128)\n",
    "    generated_comment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Generated comment: {generated_comment}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# 6. Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your dataset path\n",
    "    dataset_path = \"path/to/your/code_comment_dataset.csv\"\n",
    "    \n",
    "    # You can use a larger model with these memory savings\n",
    "    # \"Salesforce/codet5p-770m\" or even \"Salesforce/codet5p-2b\" \n",
    "    model, tokenizer = finetune_code_comment_model(\n",
    "        dataset_path=dataset_path,\n",
    "        model_name=\"Salesforce/codet5p-770m\",\n",
    "        output_dir=\"./code-comment-lora-model\"\n",
    "    )\n",
    "    \n",
    "    # Load and use the model for inference\n",
    "    load_and_use_model(\"./code-comment-lora-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5071fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/music/lib/python3.12/site-packages (2.6.0.dev20241112)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/music/lib/python3.12/site-packages (0.20.0.dev20241118)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/music/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/music/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0.dev20241112\n",
      "    Uninstalling torch-2.6.0.dev20241112:\n",
      "      Successfully uninstalled torch-2.6.0.dev20241112\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
